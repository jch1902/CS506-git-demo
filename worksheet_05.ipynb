{
  "metadata": {
    "vscode": {
      "interpreter": {
        "hash": "76ca05dc3ea24b2e3b98cdb7774adfbb40773424bf5109b477fd793f623715af"
      }
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Worksheet 05\n\nName: Jasper Hoong\nUID: U91969628\n\n### Topics\n\n- Cost Functions\n- Kmeans\n\n### Cost Function\n\nSolving Data Science problems often starts by defining a metric with which to evaluate solutions were you able to find some. This metric is called a cost function. Data Science then backtracks and tries to find a process / algorithm to find solutions that can optimize for that cost function.\n\nFor example suppose you are asked to cluster three points A, B, C into two non-empty clusters. If someone gave you the solution `{A, B}, {C}`, how would you evaluate that this is a good solution?\n\nNotice that because the clusters need to be non-empty and all points must be assigned to a cluster, it must be that two of the three points will be together in one cluster and the third will be alone in the other cluster.\n\nIn the above solution, if A and B are closer than A and C, and B and C, then this is a good solution. The smaller the distance between the two points in the same cluster (here A and B), the better the solution. So we can define our cost function to be that distance (between A and B here)!\n\nThe algorithm / process would involve clustering together the two closest points and put the third in its own cluster. This process optimizes for that cost function because no other pair of points could have a lower distance (although it could equal it).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### K means\n\na) (1-dimensional clustering) Walk through Lloyd's algorithm step by step on the following dataset:\n\n`[0, .5, 1.5, 2, 6, 6.5, 7]` (note: each of these are 1-dimensional data points)\n\nGiven the initial centroids:\n\n`[0, 2]`",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We assign all the points to the centroids 0 and 2, then we calculate the distances to find the actual centroid between all the datasets of each assigned cluster. Then, we reassign the dataset to the new closest centroids before computing the centroid of the clusters again.\n\n1)0 = [0, 0.5],2 = [1.5, 2, 6, 6.5, 7]\n2)0.25 = [0, 0.5, 1.5, 2],4.6 = [6, 6.5, 7]\n3)1 = [0, 0.5, 1.5, 2],6.5 = [6, 6.5, 7]\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "b) Describe in plain english what the cost function for k means is.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "It's the sum of the square distances of each of the points from their respective centroids",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "c) For the same number of clusters K, why could there be very different solutions to the K means algorithm on a given dataset?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There can be very different solutions to the K means algorithm on a given dataset because of the randomization of the starting centroids .",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "d) Does Lloyd's Algorithm always converge? Why / why not?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Yes, because at every step of the algorithm, we're improving the cost and we can't find a cluster that would bring us to a step we've been at before. The cost value is always decreasing.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "e) Follow along in class the implementation of Kmeans",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom PIL import Image as im\nimport matplotlib.pyplot as plt\nimport sklearn.datasets as datasets\n\ncenters = [[0, 0], [2, 2], [-3, 2], [2, -4]]\nX, _ = datasets.make_blobs(n_samples=300, centers=centers, cluster_std=1, random_state=0)\n\nclass KMeans():\n\n    def __init__(self, data, k):\n        self.data = data\n        self.k = k\n        self.assignment = [-1 for _ in range(len(data))]\n        self.snaps = []\n        \n    def distance(self, x, y):\n        return np.linalg.norm(abs(x - y))\n    \n    def snap(self, centers):\n        TEMPFILE = \"temp.png\"\n\n        fig, ax = plt.subplots()\n        ax.scatter(X[:, 0], X[:, 1], c=self.assignment)\n        ax.scatter(centers[:,0], centers[:, 1], c='r')\n        fig.savefig(TEMPFILE)\n        plt.close()\n        self.snaps.append(im.fromarray(np.asarray(im.open(TEMPFILE))))\n\n\n    def initialize(self):\n        return self.data[np.random.choice(range(len(self.data)), self.k)]\n    \n    def assign(self, centers):\n        for i in range(len(self.data)):\n            minimum = self.distance(centers[0], self.data[i])\n            self.assignment = 0\n            for j in range(i, len(centers)):\n                dist = self.distance(centers[j], self.data[i])\n                if dist < minimum:\n                    minimum = dist\n                    self.assignment[i] = j\n        return\n    \n    def is_diff_clusters(self, centers, new_centers):\n        for i in range(len(centers)):\n            if self.distance(centers[i], new_centers[i]) != 0:\n                return True\n        return False\n    \n    def get_centers(self):\n        for i in set(self.assignment):\n            cluster = []\n            for j in range(len(self.data)):\n                if self.assignment[j] == i:\n                    cluster.append(self.data[j])\n            centers.append(np.mean(cluster))\n        return centers\n            \n    \n    def lloyds(self):\n        centers = self.initialize()\n        self.assign(centers)\n        self.snap(centers)\n        new_centers = self.get_centers()\n        while self.is_diff_clusters(centers, new_centers):\n            self.assign(new_centers)\n            centers = new_centers\n            self.snap(new_centers)\n            new_centers = self.get_centers()\n            \n        \n        return\n            \n\nkmeans = KMeans(X, 6)\nkmeans.lloyds()\nimages = kmeans.snaps\n\nimages[0].save(\n    'kmeans.gif',\n    optimize=False,\n    save_all=True,\n    append_images=images[1:],\n    loop=0,\n    duration=500\n)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}