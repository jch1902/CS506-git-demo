{
  "metadata": {
    "interpreter": {
      "hash": "76ca05dc3ea24b2e3b98cdb7774adfbb40773424bf5109b477fd793f623715af"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Worksheet 12\n\nName:  Jasper Hoong\nUID: U91969628\n\n### Topics\n\n- Introduction to Classification\n- K Nearest Neighbors\n\n### Introduction to Classification\n\na) For the following examples, say whether they are or aren't an example of classification.\n\n1. Predicting whether a student will be offered a job after graduating given their GPA.\n2. Predicting how long it will take (in number of months) for a student to be offered a job after graduating, given their GPA.\n3. Predicting the number of stars (1-5) a person will assign in their yelp review given the description they wrote in the review.\n4. Predicting the number of births occuring in a specified minute.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1. Yes\n2. No\n3. Yes\n4. No",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "b) Given a dataset, how would you set things up such that you can both learn a model and get an idea of how this model might perform on data it has never seen?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can use the model to try and predict some of the data in the dataset to train the model. Separate the dataset into two sets of data, with one for setting up the model and the other for training it and testing it. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "c) In your own words, briefly explain:\n\n- underfitting\n- overfitting\n\nand what signs to look out for for each.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Underfitting is where the model makes too many mistakes and is too simple, overfitting is the opposite where it is very complex but makes little to no mistakes. A sign of underfitting if there are a lot of outliers and the opposite for overfitting.\\",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### K Nearest Neighbors",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndata = {\n    \"Attribute A\" : [3.5, 0, 1, 2.5, 2, 1.5, 2, 3.5, 1, 3, 2, 2, 2.5, 0.5, 0., 10],\n    \"Attribute B\" : [4, 1.5, 2, 1, 3.5, 2.5, 1, 0, 3, 1.5, 4, 2, 2.5, 0.5, 2.5, 10],\n    \"Class\" : [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0],\n}",
      "metadata": {},
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "a) Plot the data in a 2D plot coloring each scatter point one of two colors depending on its corresponding class.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "colors = np.array([x for x in 'bgrcmyk'])\nplt.scatter(data[\"Attribute A\"], data[\"Attribute B\"], color=colors[data[\"Class\"]].tolist())\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Outliers are points that lie far from the rest of the data. They are not necessarily invalid points however. Imagine sampling from a Normal Distribution with mean 10 and variance 1. You would expect most points you sample to be in the range [7, 13] but it's entirely possible to see 20 which, on average, should be very far from the rest of the points in the sample (unless we're VERY (un)lucky). These outliers can inhibit our ability to learn general patterns in the data since they are not representative of likely outcomes. They can still be useful in of themselves and can be analyzed in great depth depending on the problem at hand.\n\nb) Are there any points in the dataset that could be outliers? If so, please remove them from the dataset.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Calculate Q1 (25th percentile) and Q3 (75th percentile) for Attribute A and Attribute B\nQ1_A = np.percentile(data[\"Attribute A\"], 25)\nQ3_A = np.percentile(data[\"Attribute A\"], 75)\nIQR_A = Q3_A - Q1_A\n\nQ1_B = np.percentile(data[\"Attribute B\"], 25)\nQ3_B = np.percentile(data[\"Attribute B\"], 75)\nIQR_B = Q3_B - Q1_B\n\n# Define lower and upper bounds for potential outliers\nlower_bound_A = Q1_A - 1.5 * IQR_A\nupper_bound_A = Q3_A + 1.5 * IQR_A\n\nlower_bound_B = Q1_B - 1.5 * IQR_B\nupper_bound_B = Q3_B + 1.5 * IQR_B\n\n# Identify potential outliers\noutliers_A = [index for index, value in enumerate(data[\"Attribute A\"]) if value < lower_bound_A or value > upper_bound_A]\noutliers_B = [index for index, value in enumerate(data[\"Attribute B\"]) if value < lower_bound_B or value > upper_bound_B]\n\n# Remove outliers from the dataset\nfiltered_data = {\n    \"Attribute A\": [data[\"Attribute A\"][i] for i in range(len(data[\"Attribute A\"])) if i not in outliers_A],\n    \"Attribute B\": [data[\"Attribute B\"][i] for i in range(len(data[\"Attribute B\"])) if i not in outliers_B],\n    \"Class\": [data[\"Class\"][i] for i in range(len(data[\"Class\"])) if i not in outliers_A and i not in outliers_B]\n}\n\nprint(\"Filtered Data:\")\nprint(filtered_data)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Noise points are points that could be considered invalid under the general trend in the data. These could be the result of actual errors in the data or randomness that we could attribute to oversimplification (for example if missing some information / feature about each point). Considering noise points in our model can often lead to overfitting.\n\nc) Are there any points in the dataset that could be noise points?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Yes, there are",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For the following point\n\n|  A  |  B  |\n|-----|-----|\n| 0.5 |  1  |\n\nd) Plot it in a different color along with the rest of the points in the dataset.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n\n# Original filtered data (excluding outliers)\nfiltered_data = {\n    \"Attribute A\": [3.5, 1, 2.5, 2, 1.5, 2, 3.5, 1, 2, 2, 2.5, 0.5, 10],\n    \"Attribute B\": [4, 2, 1, 3.5, 2.5, 1, 0, 3, 1.5, 2, 2.5, 2.5, 10],\n    \"Class\": [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]\n}\n\n# New point to be plotted\nnew_point = [0.5, 1]\n\n# Scatter plot with different colors for each class\nplt.scatter(filtered_data[\"Attribute A\"], filtered_data[\"Attribute B\"], c=filtered_data[\"Class\"], cmap='viridis', label=\"Data Points\")\nplt.scatter(new_point[0], new_point[1], color='red', label=\"New Point (0.5, 1)\")\nplt.xlabel(\"Attribute A\")\nplt.ylabel(\"Attribute B\")\nplt.title(\"Scatter Plot of Attribute A vs Attribute B\")\nplt.colorbar(label=\"Class\")\nplt.legend()\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "e) Write a function to compute the Euclidean distance from it to all points in the dataset and pick the 3 closest points to it. In a scatter plot, draw a circle centered around the point with radius the distance of the farthest of the three points.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\n\n# Original filtered data (excluding outliers)\nfiltered_data = {\n    \"Attribute A\": [3.5, 1, 2.5, 2, 1.5, 2, 3.5, 1, 2, 2, 2.5, 0.5, 10],\n    \"Attribute B\": [4, 2, 1, 3.5, 2.5, 1, 0, 3, 1.5, 2, 2.5, 2.5, 10],\n    \"Class\": [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]\n}\n\ndef n_closest_to(example, n):\n    distances = []\n    for point in zip(filtered_data[\"Attribute A\"], filtered_data[\"Attribute B\"]):\n        distance = np.linalg.norm(np.array(example) - np.array(point))\n        distances.append(distance)\n    closest_indices = np.argsort(distances)[:n]\n    closest_points = [(filtered_data[\"Attribute A\"][i], filtered_data[\"Attribute B\"][i]) for i in closest_indices]\n    return closest_points\n\n# New point\nlocation = (0.5, 1)\n\n# Compute the 3 closest points to the new point\nclosest_points = n_closest_to(location, 3)\n\n# Compute the radius (distance to the farthest of the 3 closest points)\nradius = max([np.linalg.norm(np.array(location) - np.array(point)) for point in closest_points])\n\n# Plotting\n_, axes = plt.subplots()\naxes.scatter(filtered_data[\"Attribute A\"], filtered_data[\"Attribute B\"], c=filtered_data[\"Class\"], cmap='viridis', label=\"Data Points\")\naxes.scatter(location[0], location[1], color='red', label=\"New Point (0.5, 1)\")\ncir = Circle(location, radius, fill=False, alpha=0.8)\naxes.add_patch(cir)\naxes.set_aspect('equal') # necessary so that the circle is not oval\nplt.xlabel(\"Attribute A\")\nplt.ylabel(\"Attribute B\")\nplt.title(\"Scatter Plot with Circle\")\nplt.colorbar(label=\"Class\")\nplt.legend()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "f) Write a function that takes the three points returned by your function in e) and returns the class that the majority of points have (break ties with a deterministic default class of your choosing). Print the class assigned to this new point by your function.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def majority(points):\n    # Count the occurrences of each class in the given points\n    class_counts = {}\n    for point in points:\n        _, _, class_label = point\n        class_counts[class_label] = class_counts.get(class_label, 0) + 1\n    \n    # Determine the majority class (break ties with a default class of 1)\n    majority_class = max(class_counts, key=class_counts.get, default=1)\n    return majority_class\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "g) Re-using the functions from e) and f), you should be able to assign a class to any new point. In this exercise we will implement Leave-one-out cross validiation in order to evaluate the performance of our model.\n\nFor each point in the dataset:\n\n- consider that point as your test set and the rest of the data as your training set\n- classify that point using the training set\n- keep track of whether you were correct with the use of a counter\n\nOnce you've iterated through the entire dataset, divide the counter by the number of points in the dataset to report an overall testing accuracy.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "count = 0\n\n# Iterate through the entire dataset\nfor i in range(len(filtered_data[\"Attribute A\"])):\n    # Consider the i-th point as the test set\n    test_point = (filtered_data[\"Attribute A\"][i], filtered_data[\"Attribute B\"][i], filtered_data[\"Class\"][i])\n\n    # Create the training set by excluding the test point\n    training_data = [(filtered_data[\"Attribute A\"][j], filtered_data[\"Attribute B\"][j], filtered_data[\"Class\"][j])\n                     for j in range(len(filtered_data[\"Attribute A\"])) if j != i]\n\n    # Get the 3 closest points using the training set\n    closest_points = n_closest_to(test_point[:2], 3)\n\n    # Predict the class of the test point using the majority function\n    predicted_class = majority(closest_points)\n\n    # Compare the predicted class with the actual class\n    if predicted_class == test_point[2]:\n        count += 1\n\n# Calculate the overall accuracy\noverall_accuracy = count / len(filtered_data[\"Attribute A\"]) * 100\nprint(\"Overall accuracy =\", overall_accuracy, \"%\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}