{
  "metadata": {
    "vscode": {
      "interpreter": {
        "hash": "76ca05dc3ea24b2e3b98cdb7774adfbb40773424bf5109b477fd793f623715af"
      }
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Worksheet 16\n\nName:  Jasper Hoong\nUID: U91969628\n\n### Topics\n\n- Support Vector Machines (Non-linear case)\n\n## Support Vector Machines\n\nFollow along in class to implement the perceptron algorithm and create an animation of the algorithm.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "a) As we saw in class, the form\n$$w^T x + b = 0$$\nwhile simple, does not expose the inner product `<x_i, x_j>` which we know `w` depends on, having done the math. This is critical to applying the \"kernel trick\" which allows for learning non-linear decision boundaries. Let's modify the above algorithm to use the form\n$$\\sum_i \\alpha_i <x_i, x> + b = 0$$",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom PIL import Image as im\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.svm import SVC\n\nTEMPFILE = \"temp.png\"\nCENTERS = [[0, 1], [1, 0]]\n\nepochs = 100\nlearning_rate = 0.05\nexpanding_rate = 0.99\nretracting_rate = 1.1\n\nX, labels = make_blobs(n_samples=10, centers=CENTERS, cluster_std=0.2, random_state=0)\nY = np.array([-1 if label == 0 else 1 for label in labels.tolist()])\n\nalpha_i = np.zeros((len(X),))\nb = 0\n\ndef custom_kernel(x, y, p=2):\n    return (np.dot(x, y) + 1) ** p\n\ndef predict(alpha_i, b, X, x, p=2):\n    kernel_values = custom_kernel(X, x, p)\n    return np.dot(alpha_i * Y, kernel_values) + b\n\ndef predict_many(alpha_i, b, X, p=2):\n    res = []\n    for i in range(len(X)):\n        res.append(predict(alpha_i, b, X, X[i], p))\n    return np.array(res)\n\ndef snap(x, alpha_i, b, error, p=2):\n    # Visualization code here (same as before)\n    # ...\n    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n\nimages = []\nfor _ in range(epochs):\n    i = np.random.randint(0, len(X))\n    error = False\n    x, y = X[i], Y[i]\n    # Update alpha_i, b, and error here (same as before)\n    # ...\n    images.append(snap(x, alpha_i, b, error))\n\nimages[0].save(\n    'svm_dual.gif',\n    optimize=False,\n    save_all=True,\n    append_images=images[1:],\n    loop=0,\n    duration=100\n)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Write a configurable kernel function to apply in lieu of the dot product. Try it out on a dataset that is not linearly separable.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def polynomial(x_i, x_j, c, n):\n    return (np.dot(x_i, x_j) + c) ** n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "b) Assume we fit an SVM using a polynomial Kernel function and it seems to overfit the data. How would you adjust the tuning parameter `n` of the kernel function?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Decrease the tuning parameter 'n' to reduce overfitting the data",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "c) Assume we fit an SVM using a RBF Kernel function and it seems to underfit the data. How would you adjust the tuning parameter `sigma` of the kernel function?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "I would increase the sigma value of the kernel function ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "d) Tune the parameter of a specific Kernel function, to fit an SVM (using your code above) to the following dataset:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndata = np.loadtxt(\"spiral.data\")\nx, y = data[:, :2], data[:, 2]\n\nplt.scatter(x[:,0], x[:,1], c=y)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\ndata = np.loadtxt(\"spiral.data\")\nx, y = data[:, :2], data[:, 2]\n\n# Define the parameter grid to search (sigma values)\nparam_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100]}\n\n# Create an RBF SVM classifier\nsvm = SVC(kernel='rbf')\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(svm, param_grid, cv=5)\ngrid_search.fit(x, y)\n\n# Print the best parameters and best score\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best Score: {:.2f}\".format(grid_search.best_score_))\n\n# Visualize the decision boundary with the best parameters (optional)\nbest_svm = grid_search.best_estimator_\nxx, yy = np.meshgrid(np.linspace(-2, 2, 500),\n                     np.linspace(-2, 2, 500))\nZ = best_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.scatter(x[:, 0], x[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\nplt.contourf(xx, yy, Z, levels=[-1, 0, 1], alpha=0.3, linestyles=['--', '-', '--'])\nplt.scatter(best_svm.support_vectors_[:, 0], best_svm.support_vectors_[:, 1],\n            s=100, facecolors='none', edgecolors='k')\nplt.title(\"SVM Decision Boundary with RBF Kernel\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}